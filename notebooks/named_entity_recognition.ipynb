{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Named Entity Recognition with spaCy and HIPE Format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This recipe demonstrates how to perform Named Entity Recognition (NER) using spaCy and output results in the HIPE (Identifying Historical People, Places and other Entities) standard format. We'll walk through installing spaCy and the hipe-commons package, downloading the English language model, and extracting named entities with comprehensive HIPE-compliant annotations including IOB tagging, entity types, and additional metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rationale\n",
        "\n",
        "Named Entity Recognition is a fundamental NLP task that identifies and classifies named entities (such as persons, organizations, locations, dates, etc.) in text. The HIPE format provides a standardized, comprehensive annotation scheme that includes:\n",
        "\n",
        "- **Multi-layered annotations**: Literal vs metonymic senses\n",
        "- **Fine-grained entity types**: Detailed subtype classifications\n",
        "- **Entity linking**: Connection to knowledge bases (e.g., Wikidata)\n",
        "- **Nested entity support**: Handling complex entity structures\n",
        "- **Standardized output**: Consistent format for research and applications\n",
        "\n",
        "This recipe is essential for:\n",
        "- Historical text analysis and digital humanities research\n",
        "- Building knowledge graphs from historical documents\n",
        "- Standardized NER evaluation and comparison\n",
        "- Integration with HIPE-compliant datasets and tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Concepts\n",
        "\n",
        "**Named Entity Recognition (NER)**: The process of identifying and classifying named entities in text into predefined categories such as PERSON, ORGANIZATION, LOCATION, DATE, etc.\n",
        "\n",
        "**HIPE Format**: A comprehensive annotation standard with 10 columns per token:\n",
        "1. **TOKEN**: The actual token text\n",
        "2. **NE-COARSE-LIT**: Coarse entity type (IOB format) for literal sense\n",
        "3. **NE-COARSE-METO**: Coarse entity type for metonymic sense\n",
        "4. **NE-FINE-LIT**: Fine-grained entity type for literal sense\n",
        "5. **NE-FINE-METO**: Fine-grained entity type for metonymic sense\n",
        "6. **NE-FINE-COMP**: Component type of the entity\n",
        "7. **NE-NESTED**: Nested entity type (if any)\n",
        "8. **NEL-LIT**: Entity linking (Wikidata Q-ID) for literal sense\n",
        "9. **NEL-METO**: Entity linking for metonymic sense\n",
        "10. **MISC**: Miscellaneous flags (NoSpaceAfter, EndOfSentence, etc.)\n",
        "\n",
        "**IOB Tagging**: A labeling scheme where:\n",
        "- **B-** indicates the beginning of an entity\n",
        "- **I-** indicates inside/continuation of an entity  \n",
        "- **O** indicates outside any entity (not part of a named entity)\n",
        "\n",
        "**spaCy**: An industrial-strength natural language processing library that provides pre-trained models for various NLP tasks including NER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Overview\n",
        "\n",
        "1. **Install Dependencies**: Set up spaCy and hipe-commons libraries\n",
        "2. **Download Language Model**: Get the English language model (en_core_web_sm)\n",
        "3. **Load Model**: Initialize the spaCy pipeline\n",
        "4. **Process Text**: Apply NER and sentence segmentation\n",
        "5. **Generate HIPE Format**: Convert spaCy output to HIPE-compliant annotations\n",
        "6. **Display Results**: Show the HIPE format output with all 10 columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recipe\n",
        "\n",
        "### Step 1: Install Dependencies\n",
        "\n",
        "Install both spaCy and the hipe-commons package for HIPE format support:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Overview\n",
        "\n",
        "1. **Install spaCy**: Set up the spaCy library in your environment\n",
        "2. **Download Language Model**: Get the English language model (en_core_web_sm) that includes NER capabilities\n",
        "3. **Load Model**: Initialize the spaCy pipeline with the downloaded model\n",
        "4. **Process Text**: Apply the pipeline to identify sentence boundaries and extract entities\n",
        "5. **Generate IOB Tags**: Convert the entity annotations to IOB format for each sentence\n",
        "6. **Display Results**: Show the sentences with their corresponding IOB tags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recipe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Install spaCy\n",
        "\n",
        "First, we need to install the spaCy library. Run this command in your terminal or use the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install spaCy (uncomment if not already installed)\n",
        "# !pip install spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Download the English Language Model\n",
        "\n",
        "Download the small English model which includes NER capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the English language model (uncomment if not already downloaded)\n",
        "# !python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Load spaCy and Initialize the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy model loaded successfully!\n",
            "Model name: core_web_sm\n",
            "Model version: 3.8.0\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"spaCy model loaded successfully!\")\n",
        "print(f\"Model name: {nlp.meta['name']}\")\n",
        "print(f\"Model version: {nlp.meta['version']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Prepare Sample Text\n",
        "\n",
        "Let's use a sample text that contains various types of named entities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 files:\n",
            "- ../input/text1.txt\n",
            "- ../input/nested/text2.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "input_dir = \"../input/\"\n",
        "\n",
        "# Get all files in the input directory\n",
        "text_files = []\n",
        "for root, dirs, files in os.walk(input_dir):\n",
        "    for file in files:\n",
        "        text_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"Found {len(text_files)} files:\")\n",
        "for file in text_files:\n",
        "    print(f\"- {file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text:\n",
            "Madonna and child; the Virgin seated turned to left and seen three-quarter length, holding the infant Jesus seated on her knee and suckling him, a round composition. c.1641 Etching\n"
          ]
        }
      ],
      "source": [
        "# Sample text with various named entities\n",
        "\n",
        "with open(text_files[0], 'r') as file:\n",
        "    sample_text = file.read()\n",
        "\n",
        "print(\"Sample text:\")\n",
        "print(sample_text.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Process Text and Extract Sentences with NER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 sentences:\n",
            "1. Madonna and child; the Virgin seated turned to left and seen three-quarter length, holding the infant Jesus seated on her knee and suckling him, a round composition.\n",
            "2. c.1641\n",
            "3. Etching\n"
          ]
        }
      ],
      "source": [
        "# Process the text with spaCy\n",
        "doc = nlp(sample_text.strip())\n",
        "\n",
        "# Extract sentences\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "print(f\"Found {len(sentences)} sentences:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sent.text.strip()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Generate IOB Tags for Each Sentence\n",
        "\n",
        "Now we'll create IOB tags for each token in every sentence. To work with nested entities, we will be adopting the HIPE 2022 TSV format.\n",
        "\n",
        "```\n",
        "Each line consists of 10 columns:\n",
        "1. TOKEN: the annotated token.\n",
        "2. NE-COARSE-LIT: the coarse type (IOB-type) of the entity mention token, according to\n",
        "the literal sense.\n",
        "3. NE-COARSE-METO: the coarse type (IOB-type) of the entity mention token, according\n",
        "to the metonymic sense.\n",
        "4. NE-FINE-LIT: the fine-grained type (IOB-type.subtype.subtype) of the entity mention\n",
        "token, according to the literal sense.\n",
        "5. NE-FINE-METO: the fine-grained type (IOB-type.subtype.subtype) of the entity mention\n",
        "token, according to the metonymic sense.\n",
        "6. NE-FINE-COMP: the component type of the entity mention token.\n",
        "7. NE-NESTED: the coarse type of the nested entity (if any).\n",
        "8. NEL-LIT: the Wikidata Q id of the literal sense, or `NIL’ if an entity cannot be linked.\n",
        "Rows without link annotations have value `_’.\n",
        "9. NEL-METO: the Wikidata Q id of the metonymic sense, or `NIL’.\n",
        "10. MISC: a flag which can take the following values:\n",
        "- NoSpaceAfter: to indicate the absence of white space after the token.\n",
        "- EndOfLine: to indicate the end of a layout line.\n",
        "- EndOfSentence: to indicate the end of a sentence.\n",
        "- Partial-START:STOP: to indicate the zero-based character on-/offsets of mentions\n",
        "that do not cover the full token (esp. for German compounds). START and STOP\n",
        "follow Python's slicing semantics: \"abcd\"[1:3] means \"bc\".\n",
        "Non-specified values are marked by the underscore character “_”.\n",
        "```\n",
        "\n",
        "### Installing HIPE PyCommons\n",
        "\n",
        "```bash\n",
        "pip install git+https://github.com/hipe-eval/HIPE-pycommons\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence 1: Madonna and child; the Virgin seated turned to left and seen three-quarter length, holding the infant Jesus seated on her knee and suckling him, a round composition.\n",
            "--------------------------------------------------\n",
            "TOKEN\tNE-COARSE-LIT\tNE-COARSE-METO\tNE-FINE-LIT\tNE-FINE-METO\tNE-FINE-COMP\tNE-NESTED\tNEL-LIT\tNEL-METO\tMISC\n",
            "Madonna\tB-PERSON\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "and\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "child\t_\t_\t_\t_\t_\t_\t_\t_\tNoSpaceAfter\n",
            ";\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "the\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "Virgin\tB-PERSON\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "seated\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "turned\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "to\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "left\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "and\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "seen\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "three\tB-DATE\t_\t_\t_\t_\t_\t_\t_\tNoSpaceAfter\n",
            "-\tI-DATE\t_\t_\t_\t_\t_\t_\t_\tNoSpaceAfter\n",
            "quarter\tI-DATE\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "length\t_\t_\t_\t_\t_\t_\t_\t_\tNoSpaceAfter\n",
            ",\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "holding\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "the\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "infant\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "Jesus\tB-PERSON\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "seated\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "on\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "her\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "knee\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "and\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "suckling\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "him\t_\t_\t_\t_\t_\t_\t_\t_\tNoSpaceAfter\n",
            ",\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "a\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "round\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "composition\t_\t_\t_\t_\t_\t_\t_\t_\tNoSpaceAfter\n",
            ".\t_\t_\t_\t_\t_\t_\t_\t_\tEndOfSentence\n",
            "\n",
            "Sentence 2: c.1641\n",
            "--------------------------------------------------\n",
            "TOKEN\tNE-COARSE-LIT\tNE-COARSE-METO\tNE-FINE-LIT\tNE-FINE-METO\tNE-FINE-COMP\tNE-NESTED\tNEL-LIT\tNEL-METO\tMISC\n",
            "c.1641\t_\t_\t_\t_\t_\t_\t_\t_\tEndOfSentence\n",
            "\n",
            "Sentence 3: Etching\n",
            "--------------------------------------------------\n",
            "TOKEN\tNE-COARSE-LIT\tNE-COARSE-METO\tNE-FINE-LIT\tNE-FINE-METO\tNE-FINE-COMP\tNE-NESTED\tNEL-LIT\tNEL-METO\tMISC\n",
            "Etching\t_\t_\t_\t_\t_\t_\t_\t_\tNoSpaceAfter\n"
          ]
        }
      ],
      "source": [
        "def get_iob_tags(sentence):\n",
        "    \"\"\"\n",
        "    Generate IOB tags for tokens in a sentence following HIPE 2022 TSV format.\n",
        "    \n",
        "    Args:\n",
        "        sentence: A spaCy Span object representing a sentence\n",
        "    \n",
        "    Returns:\n",
        "        List of tuples containing the 10 HIPE columns for each token\n",
        "    \"\"\"\n",
        "    iob_tags = []\n",
        "    \n",
        "    for token in sentence:\n",
        "        # Initialize all columns with default \"_\" value\n",
        "        token_data = [\n",
        "            token.text,  # TOKEN\n",
        "            \"_\",        # NE-COARSE-LIT\n",
        "            \"_\",        # NE-COARSE-METO  \n",
        "            \"_\",        # NE-FINE-LIT\n",
        "            \"_\",        # NE-FINE-METO\n",
        "            \"_\",        # NE-FINE-COMP\n",
        "            \"_\",        # NE-NESTED\n",
        "            \"_\",        # NEL-LIT\n",
        "            \"_\",        # NEL-METO\n",
        "            \"_\"         # MISC\n",
        "        ]\n",
        "\n",
        "        # Set coarse literal IOB tag if token is part of an entity\n",
        "        if token.ent_iob_ != \"O\":\n",
        "            token_data[1] = f\"{token.ent_iob_}-{token.ent_type_}\"\n",
        "            \n",
        "        # Set EndOfSentence flag for last token\n",
        "        if token.is_sent_end:\n",
        "            token_data[9] = \"EndOfSentence\"\n",
        "            \n",
        "        # Set NoSpaceAfter flag\n",
        "        if not token.whitespace_:\n",
        "            token_data[9] = \"NoSpaceAfter\"\n",
        "            \n",
        "        iob_tags.append(token_data)\n",
        "    \n",
        "    return iob_tags\n",
        "# Process each sentence and generate IOB tags\n",
        "all_iob_tags = []\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"\\nSentence {i}: {sentence.text.strip()}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    iob_tags = get_iob_tags(sentence)\n",
        "    all_iob_tags.extend(iob_tags)\n",
        "    \n",
        "    # Display column headers\n",
        "    headers = [\"TOKEN\", \"NE-COARSE-LIT\", \"NE-COARSE-METO\", \"NE-FINE-LIT\", \n",
        "              \"NE-FINE-METO\", \"NE-FINE-COMP\", \"NE-NESTED\", \"NEL-LIT\", \n",
        "              \"NEL-METO\", \"MISC\"]\n",
        "    print(\"\\t\".join(headers))\n",
        "    \n",
        "    # Display token data\n",
        "    for token_data in iob_tags:\n",
        "        print(\"\\t\".join(token_data))\n",
        "\n",
        "# Write IOB tags to TSV file\n",
        "import os\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs('../output', exist_ok=True)\n",
        "\n",
        "# Open file and write headers\n",
        "with open('../output/sample.tsv', 'w', encoding='utf-8') as f:\n",
        "    # Write headers\n",
        "    f.write('\\t'.join(headers) + '\\n')\n",
        "    \n",
        "    # Write token data for all sentences\n",
        "    for token_data in all_iob_tags:\n",
        "        # Add EndOfLine for last token in sentence\n",
        "        if token_data[9] == \"EndOfSentence\":\n",
        "            token_data[9] = \"EndOfLine|EndOfSentence\"\n",
        "            \n",
        "        f.write('\\t'.join(token_data) + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Summary of Identified Entities\n",
        "\n",
        "Let's also create a summary of all the entities found in the text:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary of Named Entities:\n",
            "========================================\n",
            "\n",
            "PERSON:\n",
            "  - Madonna\n",
            "  - Virgin\n",
            "  - Jesus\n",
            "\n",
            "DATE:\n",
            "  - three-quarter\n",
            "\n",
            "Entity Type Explanations:\n",
            "------------------------------\n",
            "PERSON: Person names\n",
            "DATE: Dates or periods\n"
          ]
        }
      ],
      "source": [
        "# Extract and display all entities\n",
        "print(\"\\nSummary of Named Entities:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "entities_by_type = {}\n",
        "\n",
        "for ent in doc.ents:\n",
        "    entity_type = ent.label_\n",
        "    entity_text = ent.text\n",
        "    \n",
        "    if entity_type not in entities_by_type:\n",
        "        entities_by_type[entity_type] = []\n",
        "    \n",
        "    entities_by_type[entity_type].append(entity_text)\n",
        "\n",
        "# Display entities grouped by type\n",
        "for entity_type, entities in entities_by_type.items():\n",
        "    print(f\"\\n{entity_type}:\")\n",
        "    for entity in entities:\n",
        "        print(f\"  - {entity}\")\n",
        "\n",
        "# Display entity type explanations\n",
        "print(\"\\nEntity Type Explanations:\")\n",
        "print(\"-\" * 30)\n",
        "entity_explanations = {\n",
        "    'ORG': 'Organization (companies, agencies, institutions)',\n",
        "    'PERSON': 'Person names',\n",
        "    'GPE': 'Geopolitical entity (countries, cities, states)',\n",
        "    'DATE': 'Dates or periods',\n",
        "    'MONEY': 'Monetary values',\n",
        "    'CARDINAL': 'Numerals that do not fall under another type'\n",
        "}\n",
        "\n",
        "for ent_type in entities_by_type.keys():\n",
        "    explanation = entity_explanations.get(ent_type, 'See spaCy documentation for details')\n",
        "    print(f\"{ent_type}: {explanation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Madonna\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and child; the \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Virgin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " seated turned to left and seen \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    three-quarter\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " length, holding the infant \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jesus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " seated on her knee and suckling him, a round composition. c.1641 Etching</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(doc, style='ent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variations and Alternatives\n",
        "\n",
        "### Different spaCy Models\n",
        "- **en_core_web_md**: Medium model with better accuracy\n",
        "- **en_core_web_lg**: Large model with highest accuracy\n",
        "- **en_core_web_trf**: Transformer-based model with state-of-the-art performance\n",
        "\n",
        "### Alternative Libraries\n",
        "- **NLTK**: Offers basic NER capabilities with different models\n",
        "- **Hugging Face Transformers**: Provides pre-trained transformer models for NER\n",
        "- **Stanza**: Stanford's NLP library with robust NER capabilities\n",
        "\n",
        "### Custom Entity Recognition\n",
        "- Train custom spaCy models for domain-specific entities\n",
        "- Use rule-based matching for specific patterns\n",
        "- Combine multiple models for improved coverage\n",
        "\n",
        "### Output Formats\n",
        "- **BILOU tagging**: More detailed than IOB (Begin, Inside, Last, Outside, Unit)\n",
        "- **JSON format**: Structured output for API integration\n",
        "- **CoNLL format**: Standard format for NLP competitions and research\n",
        "\n",
        "### Performance Considerations\n",
        "- For large texts, process in batches using `nlp.pipe()`\n",
        "- Disable unused pipeline components to improve speed\n",
        "- Use GPU acceleration for transformer models\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gliner",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
