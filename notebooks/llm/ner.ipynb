{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402d2026",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task that involves identifying and classifying named entities (like people, places, organizations) within text. For example, in the sentence \"Shakespeare wrote Romeo and Juliet in London\", a NER system would identify \"Shakespeare\" as a person, \"Romeo and Juliet\" as a work of art, and \"London\" as a location. NER is crucial for extracting structured information from unstructured text, making it valuable for tasks like information retrieval, question answering, and metadata enrichment. In this notebook, we'll explore how to perform NER using both traditional NLP approaches and modern Large Language Models.\n",
    "\n",
    "## Rationale\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's GPT models to perform Named Entity Recognition (NER) by converting input text into annotated markdown format. Rather than using traditional NLP libraries, we leverage a Large Language Model's natural language understanding capabilities to identify and classify named entities. The notebook takes plain text as input and outputs markdown where entities are annotated in the format [Entity](TYPE), such as [London](LOCATION). This approach showcases how LLMs can be used for structured information extraction tasks in cultural heritage metadata enrichment.\n",
    "\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "1. **Text Input**: We start with plain text that needs entity recognition\n",
    "2.  **LLM Processing**: The text is sent to GPT with a prompt that instructs it to identify entities\n",
    "3. **Entity Annotation**: The LLM marks entities in markdown format: [Entity](TYPE)\n",
    "4. **Visualization**: The annotated text is displayed with color-coded entity highlighting\n",
    "\n",
    "This approach leverages the LLM's natural language understanding while producing structured, machine-readable output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f086af",
   "metadata": {},
   "source": [
    "## Necessary Functions for Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72869171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "\n",
    "def annotated_text_to_spacy_doc(text, nlp=None):\n",
    "    \"\"\"\n",
    "    Converts annotated text in format [Entity](LABEL) to a spaCy Doc with entity spans.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text with annotations like \"[Tom](PERSON) worked for [Microsoft](ORGANIZATION)\"\n",
    "        nlp (spacy.Language, optional): spaCy language model. If None, uses blank English model.\n",
    "    \n",
    "    Returns:\n",
    "        spacy.tokens.Doc: spaCy document with entity spans set\n",
    "        \n",
    "    Example:\n",
    "        >>> text = \"[Tom](PERSON) worked for [Microsoft](ORGANIZATION) in 2020 before he lived in [Rome](LOCATION).\"\n",
    "        >>> doc = annotated_text_to_spacy_doc(text)\n",
    "        >>> spacy.displacy.render(doc, style=\"ent\")\n",
    "    \"\"\"\n",
    "    if nlp is None:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "    \n",
    "    # Pattern to match [text](LABEL) format\n",
    "    pattern = r'\\[([^\\]]+)\\]\\(([^)]+)\\)'\n",
    "    \n",
    "    # Parse the text to extract tokens and entity information\n",
    "    tokens = []\n",
    "    entity_spans = []  # List of (start_token_idx, end_token_idx, label)\n",
    "    custom_labels = set()\n",
    "    \n",
    "    # Split text by the pattern and process each part\n",
    "    last_end = 0\n",
    "    token_idx = 0\n",
    "    \n",
    "    for match in re.finditer(pattern, text):\n",
    "        # Add tokens before the entity\n",
    "        before_entity = text[last_end:match.start()]\n",
    "        if before_entity.strip():\n",
    "            # Tokenize the text before the entity\n",
    "            before_tokens = before_entity.split()\n",
    "            tokens.extend(before_tokens)\n",
    "            token_idx += len(before_tokens)\n",
    "        \n",
    "        # Add the entity tokens\n",
    "        entity_text = match.group(1)\n",
    "        entity_label = match.group(2)\n",
    "        custom_labels.add(entity_label)\n",
    "        \n",
    "        # Tokenize the entity text\n",
    "        entity_tokens = entity_text.split()\n",
    "        start_token_idx = token_idx\n",
    "        tokens.extend(entity_tokens)\n",
    "        token_idx += len(entity_tokens)\n",
    "        end_token_idx = token_idx\n",
    "        \n",
    "        # Store entity span information\n",
    "        entity_spans.append((start_token_idx, end_token_idx, entity_label))\n",
    "        \n",
    "        last_end = match.end()\n",
    "    \n",
    "    # Add any remaining tokens after the last entity\n",
    "    remaining = text[last_end:]\n",
    "    if remaining.strip():\n",
    "        remaining_tokens = remaining.split()\n",
    "        tokens.extend(remaining_tokens)\n",
    "    \n",
    "    # Add custom labels to the NLP model if they don't exist\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    for label in custom_labels:\n",
    "        ner.add_label(label)\n",
    "    \n",
    "    # Create spaces array (True for tokens that should have a space after them)\n",
    "    # Simple heuristic: all tokens except the last one get a space\n",
    "    spaces = [True] * len(tokens)\n",
    "    if tokens:\n",
    "        spaces[-1] = False\n",
    "    \n",
    "    # Create the Doc from tokens\n",
    "    doc = Doc(nlp.vocab, words=tokens, spaces=spaces)\n",
    "    \n",
    "    # Create entity spans\n",
    "    entities = []\n",
    "    for start_idx, end_idx, label in entity_spans:\n",
    "        if start_idx < len(doc) and end_idx <= len(doc):\n",
    "            span = Span(doc, start_idx, end_idx, label=label)\n",
    "            entities.append(span)\n",
    "    \n",
    "    # Set entities on the document\n",
    "    doc.ents = entities\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "def visualize_annotated_text(text, nlp=None, style=\"ent\", jupyter=True):\n",
    "    \"\"\"\n",
    "    Convenience function to convert annotated text and visualize it with displaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text with annotations like \"[Tom](PERSON) worked for [Microsoft](ORGANIZATION)\"\n",
    "        nlp (spacy.Language, optional): spaCy language model. If None, uses blank English model.\n",
    "        style (str): displaCy style (\"ent\" or \"dep\")\n",
    "        jupyter (bool): Whether to render for Jupyter notebook\n",
    "    \n",
    "    Returns:\n",
    "        Rendered visualization (HTML string if not in Jupyter)\n",
    "    \"\"\"\n",
    "    doc = annotated_text_to_spacy_doc(text, nlp)\n",
    "    \n",
    "    try:\n",
    "        import spacy\n",
    "        return spacy.displacy.render(doc, style=style, jupyter=jupyter)\n",
    "    except ImportError:\n",
    "        print(\"spaCy not installed. Please install with: pip install spacy\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2697f",
   "metadata": {},
   "source": [
    "## Importing the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b14d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4c708",
   "metadata": {},
   "source": [
    "## Loading our Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63fb0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df48f5",
   "metadata": {},
   "source": [
    "## Connecting to OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626edad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d22d8a",
   "metadata": {},
   "source": [
    "## Main Variables for the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732e8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA = [{'text_original': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\",\n",
    "  'text_clean': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\",\n",
    "  'language': {'language': 'en', 'score': -868.9007034301758},\n",
    "  'sentences': [{'id': 0,\n",
    "    'start': 0,\n",
    "    'end': 130,\n",
    "    'text': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil.\"},\n",
    "   {'id': 1,\n",
    "    'start': 131,\n",
    "    'end': 324,\n",
    "    'text': 'Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.'}],\n",
    "  'tokens': [{'id': 0,\n",
    "    'text': 'This',\n",
    "    'start': 0,\n",
    "    'end': 4,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 1,\n",
    "    'text': 'painting',\n",
    "    'start': 5,\n",
    "    'end': 13,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 2,\n",
    "    'text': 'depicts',\n",
    "    'start': 14,\n",
    "    'end': 21,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 3,\n",
    "    'text': 'Monet',\n",
    "    'start': 22,\n",
    "    'end': 27,\n",
    "    'ws': False,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 4,\n",
    "    'text': \"'s\",\n",
    "    'start': 27,\n",
    "    'end': 29,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 5,\n",
    "    'text': 'first',\n",
    "    'start': 30,\n",
    "    'end': 35,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 6,\n",
    "    'text': 'wife',\n",
    "    'start': 36,\n",
    "    'end': 40,\n",
    "    'ws': False,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 7,\n",
    "    'text': ',',\n",
    "    'start': 40,\n",
    "    'end': 41,\n",
    "    'ws': True,\n",
    "    'is_punct': True,\n",
    "    'sent_id': 0},\n",
    "   {'id': 8,\n",
    "    'text': 'Camille',\n",
    "    'start': 42,\n",
    "    'end': 49,\n",
    "    'ws': False,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 9,\n",
    "    'text': ',',\n",
    "    'start': 49,\n",
    "    'end': 50,\n",
    "    'ws': True,\n",
    "    'is_punct': True,\n",
    "    'sent_id': 0},\n",
    "   {'id': 10,\n",
    "    'text': 'outside',\n",
    "    'start': 51,\n",
    "    'end': 58,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 11,\n",
    "    'text': 'on',\n",
    "    'start': 59,\n",
    "    'end': 61,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 12,\n",
    "    'text': 'a',\n",
    "    'start': 62,\n",
    "    'end': 63,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 13,\n",
    "    'text': 'snowy',\n",
    "    'start': 64,\n",
    "    'end': 69,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 14,\n",
    "    'text': 'day',\n",
    "    'start': 70,\n",
    "    'end': 73,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 15,\n",
    "    'text': 'passing',\n",
    "    'start': 74,\n",
    "    'end': 81,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 16,\n",
    "    'text': 'by',\n",
    "    'start': 82,\n",
    "    'end': 84,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 17,\n",
    "    'text': 'the',\n",
    "    'start': 85,\n",
    "    'end': 88,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 18,\n",
    "    'text': 'French',\n",
    "    'start': 89,\n",
    "    'end': 95,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 19,\n",
    "    'text': 'doors',\n",
    "    'start': 96,\n",
    "    'end': 101,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 20,\n",
    "    'text': 'of',\n",
    "    'start': 102,\n",
    "    'end': 104,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 21,\n",
    "    'text': 'their',\n",
    "    'start': 105,\n",
    "    'end': 110,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 22,\n",
    "    'text': 'home',\n",
    "    'start': 111,\n",
    "    'end': 115,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 23,\n",
    "    'text': 'at',\n",
    "    'start': 116,\n",
    "    'end': 118,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 24,\n",
    "    'text': 'Argenteuil',\n",
    "    'start': 119,\n",
    "    'end': 129,\n",
    "    'ws': False,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 0},\n",
    "   {'id': 25,\n",
    "    'text': '.',\n",
    "    'start': 129,\n",
    "    'end': 130,\n",
    "    'ws': True,\n",
    "    'is_punct': True,\n",
    "    'sent_id': 0},\n",
    "   {'id': 26,\n",
    "    'text': 'Her',\n",
    "    'start': 131,\n",
    "    'end': 134,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 27,\n",
    "    'text': 'face',\n",
    "    'start': 135,\n",
    "    'end': 139,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 28,\n",
    "    'text': 'is',\n",
    "    'start': 140,\n",
    "    'end': 142,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 29,\n",
    "    'text': 'rendered',\n",
    "    'start': 143,\n",
    "    'end': 151,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 30,\n",
    "    'text': 'in',\n",
    "    'start': 152,\n",
    "    'end': 154,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 31,\n",
    "    'text': 'a',\n",
    "    'start': 155,\n",
    "    'end': 156,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 32,\n",
    "    'text': 'radically',\n",
    "    'start': 157,\n",
    "    'end': 166,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 33,\n",
    "    'text': 'bold',\n",
    "    'start': 167,\n",
    "    'end': 171,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 34,\n",
    "    'text': 'Impressionist',\n",
    "    'start': 172,\n",
    "    'end': 185,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 35,\n",
    "    'text': 'technique',\n",
    "    'start': 186,\n",
    "    'end': 195,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 36,\n",
    "    'text': 'of',\n",
    "    'start': 196,\n",
    "    'end': 198,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 37,\n",
    "    'text': 'mere',\n",
    "    'start': 199,\n",
    "    'end': 203,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 38,\n",
    "    'text': 'daubs',\n",
    "    'start': 204,\n",
    "    'end': 209,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 39,\n",
    "    'text': 'of',\n",
    "    'start': 210,\n",
    "    'end': 212,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 40,\n",
    "    'text': 'paint',\n",
    "    'start': 213,\n",
    "    'end': 218,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 41,\n",
    "    'text': 'quickly',\n",
    "    'start': 219,\n",
    "    'end': 226,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 42,\n",
    "    'text': 'applied',\n",
    "    'start': 227,\n",
    "    'end': 234,\n",
    "    'ws': False,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 43,\n",
    "    'text': ',',\n",
    "    'start': 234,\n",
    "    'end': 235,\n",
    "    'ws': True,\n",
    "    'is_punct': True,\n",
    "    'sent_id': 1},\n",
    "   {'id': 44,\n",
    "    'text': 'just',\n",
    "    'start': 236,\n",
    "    'end': 240,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 45,\n",
    "    'text': 'as',\n",
    "    'start': 241,\n",
    "    'end': 243,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 46,\n",
    "    'text': 'the',\n",
    "    'start': 244,\n",
    "    'end': 247,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 47,\n",
    "    'text': 'snow',\n",
    "    'start': 248,\n",
    "    'end': 252,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 48,\n",
    "    'text': 'and',\n",
    "    'start': 253,\n",
    "    'end': 256,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 49,\n",
    "    'text': 'trees',\n",
    "    'start': 257,\n",
    "    'end': 262,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 50,\n",
    "    'text': 'are',\n",
    "    'start': 263,\n",
    "    'end': 266,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 51,\n",
    "    'text': 'defined',\n",
    "    'start': 267,\n",
    "    'end': 274,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 52,\n",
    "    'text': 'by',\n",
    "    'start': 275,\n",
    "    'end': 277,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 53,\n",
    "    'text': 'broad',\n",
    "    'start': 278,\n",
    "    'end': 283,\n",
    "    'ws': False,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 54,\n",
    "    'text': ',',\n",
    "    'start': 283,\n",
    "    'end': 284,\n",
    "    'ws': True,\n",
    "    'is_punct': True,\n",
    "    'sent_id': 1},\n",
    "   {'id': 55,\n",
    "    'text': 'broken',\n",
    "    'start': 285,\n",
    "    'end': 291,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 56,\n",
    "    'text': 'strokes',\n",
    "    'start': 292,\n",
    "    'end': 299,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 57,\n",
    "    'text': 'of',\n",
    "    'start': 300,\n",
    "    'end': 302,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 58,\n",
    "    'text': 'pure',\n",
    "    'start': 303,\n",
    "    'end': 307,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 59,\n",
    "    'text': 'white',\n",
    "    'start': 308,\n",
    "    'end': 313,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 60,\n",
    "    'text': 'and',\n",
    "    'start': 314,\n",
    "    'end': 317,\n",
    "    'ws': True,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 61,\n",
    "    'text': 'green',\n",
    "    'start': 318,\n",
    "    'end': 323,\n",
    "    'ws': False,\n",
    "    'is_punct': False,\n",
    "    'sent_id': 1},\n",
    "   {'id': 62,\n",
    "    'text': '.',\n",
    "    'start': 323,\n",
    "    'end': 324,\n",
    "    'ws': False,\n",
    "    'is_punct': True,\n",
    "    'sent_id': 1}],\n",
    "  'meta': {'source': 'CMA',\n",
    "   'id': 135382,\n",
    "   'char_count': 324,\n",
    "   'token_count': 63,\n",
    "   'sentence_count': 2}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e86d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "LABELS = [\"PERSON\", \"LOCATION\", \"ORGANIZATION\"]\n",
    "TEXT = INPUT_DATA[0][\"text_clean\"]\n",
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af9333",
   "metadata": {},
   "source": [
    "## Creating the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94acadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Convert the following text into a structured markdown format, where you annotate the entities in the text in the following format: [Tom](PERSON) went to [New York](PLACE).\n",
    "\n",
    "Look for the following entities types:\n",
    "{LABELS}\n",
    "\n",
    "Do this for the following text:\n",
    "{TEXT}\n",
    "\n",
    "Only return the markdown output, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9bac810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert the following text into a structured markdown format, where you annotate the entities in the text in the following format: [Tom](PERSON) went to [New York](PLACE).\n",
      "\n",
      "Look for the following entities types:\n",
      "['PERSON', 'LOCATION', 'ORGANIZATION']\n",
      "\n",
      "Do this for the following text:\n",
      "This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\n",
      "\n",
      "Only return the markdown output, nothing else.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15cf555",
   "metadata": {},
   "source": [
    "## Calling OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30523c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc94228",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_output = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dd03da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This painting depicts [Monet](PERSON)'s first wife, [Camille](PERSON), outside on a snowy day passing by the French doors of their home at [Argenteuil](LOCATION). Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\n"
     ]
    }
   ],
   "source": [
    "print(markdown_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b974e5",
   "metadata": {},
   "source": [
    "## Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e1fea3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This painting depicts \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Monet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " 's first wife, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Camille\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " , outside on a snowy day passing by the French doors of their home at \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Argenteuil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOCATION</span>\n",
       "</mark>\n",
       " . Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_annotated_text(markdown_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83f8ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Monet, Camille, Argenteuil)\n"
     ]
    }
   ],
   "source": [
    "doc = annotated_text_to_spacy_doc(markdown_output)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b34387e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monet PERSON 22 27\n",
      "Camille PERSON 43 50\n",
      "Argenteuil LOCATION 121 131\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "    entities.append({\n",
    "        \"text\": ent.text,\n",
    "        \"label\": ent.label_,\n",
    "        \"start_char\": ent.start_char,\n",
    "        \"end_char\": ent.end_char\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "363bac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA[0][\"entities\"] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Monet', 'label': 'PERSON', 'start_char': 22, 'end_char': 27}, {'text': 'Camille', 'label': 'PERSON', 'start_char': 43, 'end_char': 50}, {'text': 'Argenteuil', 'label': 'LOCATION', 'start_char': 121, 'end_char': 131}]\n"
     ]
    }
   ],
   "source": [
    "print(INPUT_DATA[0][\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815b05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gliner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
